{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUSHI\\Anaconda Python\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\RUSHI\\Anaconda Python\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\RUSHI\\Anaconda Python\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\RUSHI\\Anaconda Python\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\RUSHI\\Anaconda Python\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\RUSHI\\Anaconda Python\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score,  f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback\n",
    "import keras\n",
    "\n",
    "import pydot\n",
    "import graphviz\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.5\n",
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tcga_df = pd.read_csv('D://GitHub/Gene-compression-and-Cancer-type-classification/data/train_tcga_expression_matrix_processed.tsv', header=0, sep='\\t')\n",
    "test_tcga_df = pd.read_csv('D://GitHub/Gene-compression-and-Cancer-type-classification/data/test_tcga_expression_matrix_processed.tsv', header=0, sep='\\t')\n",
    "\n",
    "labels_tcga_df = pd.read_csv('D://GitHub/Gene-compression-and-Cancer-type-classification/data/tcga_sample_identifiers.tsv', header=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train_tcga_df['sample_id']\n",
    "test_id = test_tcga_df['sample_id']\n",
    "label_id = labels_tcga_df['sample_id']\n",
    "\n",
    "merge_train = []\n",
    "merge_test = []\n",
    "\n",
    "for i in train_id:\n",
    "    val = labels_tcga_df.loc[labels_tcga_df['sample_id'] == i]\n",
    "    merge_train.append(str(val['cancer_type']).split()[1])\n",
    "    \n",
    "for i in test_id:\n",
    "    val = labels_tcga_df.loc[labels_tcga_df['sample_id'] == i]\n",
    "    merge_test.append(str(val['cancer_type']).split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>10001</th>\n",
       "      <th>10002</th>\n",
       "      <th>10003</th>\n",
       "      <th>100037417</th>\n",
       "      <th>...</th>\n",
       "      <th>9988</th>\n",
       "      <th>9989</th>\n",
       "      <th>999</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9997</th>\n",
       "      <th>cancer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-LL-A73Z-01</td>\n",
       "      <td>202.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>329.0</td>\n",
       "      <td>84.5</td>\n",
       "      <td>492.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>4.590</td>\n",
       "      <td>14.70</td>\n",
       "      <td>337.0</td>\n",
       "      <td>...</td>\n",
       "      <td>717.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>6360.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>10.60</td>\n",
       "      <td>3190.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>892.0</td>\n",
       "      <td>BRCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-55-8207-01</td>\n",
       "      <td>77.5</td>\n",
       "      <td>22.5</td>\n",
       "      <td>74.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>784.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>2.540</td>\n",
       "      <td>176.00</td>\n",
       "      <td>153.0</td>\n",
       "      <td>...</td>\n",
       "      <td>923.0</td>\n",
       "      <td>2490.0</td>\n",
       "      <td>11300.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>4030.0</td>\n",
       "      <td>9.08</td>\n",
       "      <td>2890.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>LUAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-FF-A7CR-01</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>486.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.47</td>\n",
       "      <td>348.0</td>\n",
       "      <td>...</td>\n",
       "      <td>897.0</td>\n",
       "      <td>861.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>464.0</td>\n",
       "      <td>3320.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1330.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>DLBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-BK-A13C-11</td>\n",
       "      <td>80.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>70.6</td>\n",
       "      <td>284.0</td>\n",
       "      <td>2420.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>1.200</td>\n",
       "      <td>91.40</td>\n",
       "      <td>231.0</td>\n",
       "      <td>...</td>\n",
       "      <td>737.0</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>5.24</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>UCEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-EB-A6L9-06</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>0.945</td>\n",
       "      <td>2.36</td>\n",
       "      <td>585.0</td>\n",
       "      <td>...</td>\n",
       "      <td>328.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>7010.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>10.90</td>\n",
       "      <td>3780.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>SKCM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sample_id      1    10     100   1000   10000  10001  10002   10003  \\\n",
       "0  TCGA-LL-A73Z-01  202.0  28.5   329.0   84.5   492.0  448.0  4.590   14.70   \n",
       "1  TCGA-55-8207-01   77.5  22.5    74.5   13.1   784.0  333.0  2.540  176.00   \n",
       "2  TCGA-FF-A7CR-01  152.0   0.0  3020.0   26.6   486.0  497.0  0.000    8.47   \n",
       "3  TCGA-BK-A13C-11   80.5  40.0    70.6  284.0  2420.0  325.0  1.200   91.40   \n",
       "4  TCGA-EB-A6L9-06  319.0   0.0   422.0  184.0   423.0  392.0  0.945    2.36   \n",
       "\n",
       "   100037417  ...   9988    9989      999    9990    9991   9992    9993  \\\n",
       "0      337.0  ...  717.0  1800.0   6360.0   299.0  2310.0  10.60  3190.0   \n",
       "1      153.0  ...  923.0  2490.0  11300.0  1150.0  4030.0   9.08  2890.0   \n",
       "2      348.0  ...  897.0   861.0     39.7   464.0  3320.0   0.00  1330.0   \n",
       "3      231.0  ...  737.0  1410.0     10.9  1120.0  1990.0   5.24  3090.0   \n",
       "4      585.0  ...  328.0  1340.0   7010.0   450.0   563.0  10.90  3780.0   \n",
       "\n",
       "    9994    9997  cancer_type  \n",
       "0  337.0   892.0         BRCA  \n",
       "1  316.0   301.0         LUAD  \n",
       "2  606.0   558.0         DLBC  \n",
       "3  673.0   263.0         UCEC  \n",
       "4   37.3  1120.0         SKCM  \n",
       "\n",
       "[5 rows x 16150 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tcga_df['cancer_type'] = merge_train\n",
    "train_tcga_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>10001</th>\n",
       "      <th>10002</th>\n",
       "      <th>10003</th>\n",
       "      <th>100037417</th>\n",
       "      <th>...</th>\n",
       "      <th>9988</th>\n",
       "      <th>9989</th>\n",
       "      <th>999</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9997</th>\n",
       "      <th>cancer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA-CN-5365-01</td>\n",
       "      <td>70.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>18.70</td>\n",
       "      <td>73.9</td>\n",
       "      <td>757.0</td>\n",
       "      <td>1.82</td>\n",
       "      <td>10.50</td>\n",
       "      <td>414.0</td>\n",
       "      <td>...</td>\n",
       "      <td>612.0</td>\n",
       "      <td>5780.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>3830.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4220.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>HNSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA-LP-A7HU-01</td>\n",
       "      <td>27.8</td>\n",
       "      <td>0.949</td>\n",
       "      <td>517.0</td>\n",
       "      <td>6.17</td>\n",
       "      <td>63.6</td>\n",
       "      <td>365.0</td>\n",
       "      <td>4.27</td>\n",
       "      <td>9.02</td>\n",
       "      <td>633.0</td>\n",
       "      <td>...</td>\n",
       "      <td>543.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>4960.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>2220.0</td>\n",
       "      <td>17.60</td>\n",
       "      <td>2410.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>993.0</td>\n",
       "      <td>CESC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA-22-5491-11</td>\n",
       "      <td>87.6</td>\n",
       "      <td>3.760</td>\n",
       "      <td>88.3</td>\n",
       "      <td>14.40</td>\n",
       "      <td>642.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>6.84</td>\n",
       "      <td>59.90</td>\n",
       "      <td>159.0</td>\n",
       "      <td>...</td>\n",
       "      <td>479.0</td>\n",
       "      <td>2190.0</td>\n",
       "      <td>4480.0</td>\n",
       "      <td>870.0</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>5.13</td>\n",
       "      <td>5030.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>LUSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA-CS-6667-01</td>\n",
       "      <td>75.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>55.6</td>\n",
       "      <td>2720.00</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>3.90</td>\n",
       "      <td>9.57</td>\n",
       "      <td>251.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>23.70</td>\n",
       "      <td>7530.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA-20-1684-01</td>\n",
       "      <td>63.1</td>\n",
       "      <td>0.703</td>\n",
       "      <td>75.2</td>\n",
       "      <td>4500.00</td>\n",
       "      <td>792.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>1.64</td>\n",
       "      <td>4.16</td>\n",
       "      <td>623.0</td>\n",
       "      <td>...</td>\n",
       "      <td>812.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>8410.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>14.80</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>OV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sample_id     1     10     100     1000   10000  10001  10002  10003  \\\n",
       "0  TCGA-CN-5365-01  70.5  0.000  1310.0    18.70    73.9  757.0   1.82  10.50   \n",
       "1  TCGA-LP-A7HU-01  27.8  0.949   517.0     6.17    63.6  365.0   4.27   9.02   \n",
       "2  TCGA-22-5491-11  87.6  3.760    88.3    14.40   642.0  295.0   6.84  59.90   \n",
       "3  TCGA-CS-6667-01  75.8  0.000    55.6  2720.00  2170.0  281.0   3.90   9.57   \n",
       "4  TCGA-20-1684-01  63.1  0.703    75.2  4500.00   792.0  433.0   1.64   4.16   \n",
       "\n",
       "   100037417  ...    9988    9989      999    9990    9991   9992    9993  \\\n",
       "0      414.0  ...   612.0  5780.0  10400.0   879.0  3830.0   4.10  4220.0   \n",
       "1      633.0  ...   543.0  1360.0   4960.0   510.0  2220.0  17.60  2410.0   \n",
       "2      159.0  ...   479.0  2190.0   4480.0   870.0  2120.0   5.13  5030.0   \n",
       "3      251.0  ...  1550.0  1370.0     12.8  1430.0   601.0  23.70  7530.0   \n",
       "4      623.0  ...   812.0  1070.0   8410.0   505.0  1170.0  14.80  1910.0   \n",
       "\n",
       "    9994    9997  cancer_type  \n",
       "0  308.0  1300.0         HNSC  \n",
       "1  233.0   993.0         CESC  \n",
       "2  285.0   530.0         LUSC  \n",
       "3  473.0   258.0          LGG  \n",
       "4  281.0   221.0           OV  \n",
       "\n",
       "[5 rows x 16150 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tcga_df['cancer_type'] = merge_test\n",
    "test_tcga_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    \n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, var_layer, mean_layer, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        self.var_layer = var_layer\n",
    "        self.mean_layer = mean_layer\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + self.var_layer - K.square(self.mean_layer) - \n",
    "                                K.exp(self.var_layer), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tybalt():\n",
    "    \"\"\"\n",
    "    Facilitates the training and output of tybalt model trained on TCGA RNAseq gene expression data\n",
    "    \"\"\"\n",
    "    def __init__(self, original_dim, hidden_dim, latent_dim,\n",
    "                 batch_size, epochs, learning_rate, kappa, beta):\n",
    "        self.original_dim = original_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.kappa = kappa\n",
    "        self.beta = beta\n",
    "\n",
    "    def build_encoder_layer(self):\n",
    "        # Input place holder for RNAseq data with specific input size\n",
    "        self.rnaseq_input = Input(shape=(self.original_dim, ))\n",
    "\n",
    "        # Input layer is compressed into a mean and log variance vector of size `latent_dim`\n",
    "        # Each layer is initialized with glorot uniform weights and each step (dense connections, batch norm,\n",
    "        # and relu activation) are funneled separately\n",
    "        # Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "        hidden_dense_linear = Dense(self.hidden_dim, kernel_initializer='glorot_uniform')(self.rnaseq_input)\n",
    "        hidden_dense_batchnorm = BatchNormalization()(hidden_dense_linear)\n",
    "        hidden_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "        z_mean_dense_linear = Dense(self.latent_dim, kernel_initializer='glorot_uniform')(hidden_encoded)\n",
    "        z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "        self.z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "        z_log_var_dense_linear = Dense(self.latent_dim, kernel_initializer='glorot_uniform')(hidden_encoded)\n",
    "        z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "        self.z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "        # return the encoded and randomly sampled z vector\n",
    "        # Takes two keras layers as input to the custom sampling function layer with a `latent_dim` output\n",
    "        self.z = Lambda(sampling, output_shape=(self.latent_dim, ))([self.z_mean_encoded, self.z_log_var_encoded])\n",
    "    \n",
    "    def build_decoder_layer(self):\n",
    "        # The decoding layer is much simpler with a single layer glorot uniform initialized and sigmoid activation\n",
    "        self.decoder_model = Sequential()\n",
    "        self.decoder_model.add(Dense(self.hidden_dim, activation='relu', input_dim=self.latent_dim))\n",
    "        self.decoder_model.add(Dense(self.original_dim, activation='sigmoid'))\n",
    "        self.rnaseq_reconstruct = self.decoder_model(self.z)\n",
    "        \n",
    "    def compile_vae(self):\n",
    "        adam = optimizers.Adam(lr=self.learning_rate)\n",
    "        vae_layer = CustomVariationalLayer(self.z_log_var_encoded,\n",
    "                                           self.z_mean_encoded)([self.rnaseq_input, self.rnaseq_reconstruct])\n",
    "        self.vae = Model(self.rnaseq_input, vae_layer)\n",
    "        self.vae.compile(optimizer=adam, loss=None, loss_weights=[self.beta])\n",
    "        \n",
    "    def get_summary(self):\n",
    "        self.vae.summary()\n",
    "  \n",
    "    def visualize_architecture(self, output_file):\n",
    "        # Visualize the connections of the custom VAE model\n",
    "        plot_model(self.vae, to_file=output_file)\n",
    "        SVG(model_to_dot(self.vae).create(prog='dot', format='svg'))\n",
    "        \n",
    "    def train_vae(self):\n",
    "        self.hist = self.vae.fit(np.array(rnaseq_train_df),\n",
    "               shuffle=True,\n",
    "               epochs=self.epochs,\n",
    "               batch_size=self.batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), np.array(rnaseq_test_df)),\n",
    "               callbacks=[WarmUpCallback(self.beta, self.kappa)])\n",
    "    \n",
    "    def visualize_training(self, output_file):\n",
    "        # Visualize training performance\n",
    "        history_df = pd.DataFrame(self.hist.history)\n",
    "        ax = history_df.plot()\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('VAE Loss')\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig(output_file)\n",
    "        \n",
    "    def compress(self, df):\n",
    "        # Model to compress input\n",
    "        self.encoder = Model(self.rnaseq_input, self.z_mean_encoded)\n",
    "        \n",
    "        # Encode rnaseq into the hidden/latent representation - and save output\n",
    "        encoded_df = self.encoder.predict_on_batch(df)\n",
    "        encoded_df = pd.DataFrame(encoded_df, columns=range(1, self.latent_dim + 1),\n",
    "                                  index=rnaseq_df.index)\n",
    "        return encoded_df\n",
    "    \n",
    "    def get_decoder_weights(self):\n",
    "        # build a generator that can sample from the learned distribution\n",
    "        decoder_input = Input(shape=(self.latent_dim, ))  # can generate from any sampled z vector\n",
    "        _x_decoded_mean = self.decoder_model(decoder_input)\n",
    "        self.decoder = Model(decoder_input, _x_decoded_mean)\n",
    "        weights = []\n",
    "        for layer in self.decoder.layers:\n",
    "            weights.append(layer.get_weights())\n",
    "        return(weights)\n",
    "    \n",
    "    def predict(self, df):\n",
    "        return self.decoder.predict(np.array(df))\n",
    "    \n",
    "    def save_models(self, encoder_file, decoder_file):\n",
    "        self.encoder.save(encoder_file)\n",
    "        self.decoder.save(decoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set common hyper parameters\n",
    "rnaseq_train_df = train_tcga_df.drop(['cancer_type', 'sample_id'], axis=1)\n",
    "rnaseq_test_df = test_tcga_df.drop(['cancer_type', 'sample_id'], axis=1)\n",
    "\n",
    "original_dim = rnaseq_train_df.shape[1]\n",
    "latent_dim = 100\n",
    "beta = K.variable(0)\n",
    "epsilon_std = 1.0\n",
    "\n",
    "# Model A (100 hidden layer size)\n",
    "model_a_latent_dim = 100\n",
    "model_a_batch_size = 100\n",
    "model_a_epochs = 100\n",
    "model_a_learning_rate = 0.001\n",
    "model_a_kappa = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = Tybalt(original_dim=original_dim,\n",
    "                 hidden_dim=model_a_latent_dim,\n",
    "                 latent_dim=latent_dim,\n",
    "                 batch_size=model_a_batch_size,\n",
    "                 epochs=model_a_epochs,\n",
    "                 learning_rate=model_a_learning_rate,\n",
    "                 kappa=model_a_kappa,\n",
    "                 beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 16148)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           1614900     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 100)           400         dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 100)           0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 100)           10100       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 100)           10100       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 100)           400         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 100)           400         dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 100)           0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 100)           0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 100)           0           activation_2[0][0]               \n",
      "                                                                   activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)        (None, 16148)         1641048     lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "custom_variational_layer_1 (Cust [(None, 16148), (None 0           input_1[0][0]                    \n",
      "                                                                   sequential_1[1][0]               \n",
      "====================================================================================================\n",
      "Total params: 3,277,348\n",
      "Trainable params: 3,276,748\n",
      "Non-trainable params: 600\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RUSHI\\Anaconda Python\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# Compile Model A\n",
    "model_a.build_encoder_layer()\n",
    "model_a.build_decoder_layer()\n",
    "model_a.compile_vae()\n",
    "model_a.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9954 samples, validate on 1106 samples\n",
      "Epoch 1/100\n",
      "9954/9954 [==============================] - 18s - loss: -187824271.3552 - val_loss: -290844628.7740\n",
      "Epoch 2/100\n",
      "9954/9954 [==============================] - 18s - loss: -313498801.0512 - val_loss: -311221288.1013\n",
      "Epoch 3/100\n",
      "9954/9954 [==============================] - 18s - loss: -314164426.7567 - val_loss: -314528993.3888\n",
      "Epoch 4/100\n",
      "9954/9954 [==============================] - 18s - loss: -314419142.6996 - val_loss: -314723176.1591\n",
      "Epoch 5/100\n",
      "9954/9954 [==============================] - 17s - loss: -314528000.8037 - val_loss: -314813502.2061\n",
      "Epoch 6/100\n",
      "9954/9954 [==============================] - 18s - loss: -314502608.2218 - val_loss: -314764882.8065\n",
      "Epoch 7/100\n",
      "9954/9954 [==============================] - 18s - loss: -314588241.4563 - val_loss: -314836854.2785\n",
      "Epoch 8/100\n",
      "9954/9954 [==============================] - 18s - loss: -314626403.0283 - val_loss: -314567510.3363\n",
      "Epoch 9/100\n",
      "9954/9954 [==============================] - 18s - loss: -314627967.6078 - val_loss: -314690507.8047\n",
      "Epoch 10/100\n",
      "9954/9954 [==============================] - 18s - loss: -314645605.3751 - val_loss: -314605288.1013\n",
      "Epoch 11/100\n",
      "9954/9954 [==============================] - 18s - loss: -314635919.0452 - val_loss: -314852373.9892\n",
      "Epoch 12/100\n",
      "9954/9954 [==============================] - 18s - loss: -314668780.0362 - val_loss: -314852336.3761\n",
      "Epoch 13/100\n",
      "9954/9954 [==============================] - 18s - loss: -314647977.6894 - val_loss: -314851754.9946\n",
      "Epoch 14/100\n",
      "9954/9954 [==============================] - 18s - loss: -314665811.8288 - val_loss: -314850883.9349\n",
      "Epoch 15/100\n",
      "9954/9954 [==============================] - 18s - loss: -314643264.0964 - val_loss: -314850918.8282\n",
      "Epoch 16/100\n",
      "9954/9954 [==============================] - 19s - loss: -314673441.9096 - val_loss: -314852463.6817\n",
      "Epoch 17/100\n",
      "9954/9954 [==============================] - 19s - loss: -314678317.6436 - val_loss: -314852472.3617\n",
      "Epoch 18/100\n",
      "9954/9954 [==============================] - 19s - loss: -314665480.0434 - val_loss: -314852498.4014\n",
      "Epoch 19/100\n",
      "9954/9954 [==============================] - 18s - loss: -314675901.6725 - val_loss: -314852507.0814\n",
      "Epoch 20/100\n",
      "9954/9954 [==============================] - 18s - loss: -314661982.3733 - val_loss: -314636576.5208\n",
      "Epoch 21/100\n",
      "9954/9954 [==============================] - 18s - loss: -314666146.0832 - val_loss: -314780770.1989\n",
      "Epoch 22/100\n",
      "9954/9954 [==============================] - 18s - loss: -314650908.8366 - val_loss: -314852512.6944\n",
      "Epoch 23/100\n",
      "9954/9954 [==============================] - 18s - loss: -314671972.5650 - val_loss: -314851864.9403\n",
      "Epoch 24/100\n",
      "9954/9954 [==============================] - 18s - loss: -314681990.6353 - val_loss: -314852096.2315\n",
      "Epoch 25/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687224.4581 - val_loss: -314852463.85534701829.\n",
      "Epoch 26/100\n",
      "9954/9954 [==============================] - 18s - loss: -314685462.6064 - val_loss: -314852148.4846\n",
      "Epoch 27/100\n",
      "9954/9954 [==============================] - 18s - loss: -314674950.3653 - val_loss: -314852310.5099\n",
      "Epoch 28/100\n",
      "9954/9954 [==============================] - 18s - loss: -314680369.2956 - val_loss: -314852495.6817\n",
      "Epoch 29/100\n",
      "9954/9954 [==============================] - 19s - loss: -314677081.2489 - val_loss: -314839111.0597\n",
      "Epoch 30/100\n",
      "9954/9954 [==============================] - 18s - loss: -314674788.8093 - val_loss: -314848005.2658\n",
      "Epoch 31/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687706.8756 - val_loss: -314852504.3617\n",
      "Epoch 32/100\n",
      "9954/9954 [==============================] - 20s - loss: -314686374.3074 - val_loss: -314850690.2568\n",
      "Epoch 33/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687496.1398 - val_loss: -314852507.2550\n",
      "Epoch 34/100\n",
      "9954/9954 [==============================] - 19s - loss: -314676998.6803 - val_loss: -314806480.5497\n",
      "Epoch 35/100\n",
      "9954/9954 [==============================] - 20s - loss: -314682767.8103 - val_loss: -314852458.0687\n",
      "Epoch 36/100\n",
      "9954/9954 [==============================] - 20s - loss: -314680120.1881 - val_loss: -314852504.1881\n",
      "Epoch 37/100\n",
      "9954/9954 [==============================] - 20s - loss: -314685292.3834 - val_loss: -314848002.5461\n",
      "Epoch 38/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687329.8196 - val_loss: -314852510.1483\n",
      "Epoch 39/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687733.6870 - val_loss: -314852518.8282\n",
      "Epoch 40/100\n",
      "9954/9954 [==============================] - 20s - loss: -314682474.7631 - val_loss: -314852348.1230\n",
      "Epoch 41/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687736.0016 - val_loss: -314852518.8282\n",
      "Epoch 42/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687735.9373 - val_loss: -314852519.0018\n",
      "Epoch 43/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687727.4117 - val_loss: -314852507.2550\n",
      "Epoch 44/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687732.5554 - val_loss: -314852513.0416\n",
      "Epoch 45/100\n",
      "9954/9954 [==============================] - 20s - loss: -314679153.0191 - val_loss: -314852515.9349\n",
      "Epoch 46/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687728.7619 - val_loss: -314852513.0416\n",
      "Epoch 47/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687368.6156 - val_loss: -314852521.8951\n",
      "Epoch 48/100\n",
      "9954/9954 [==============================] - 20s - loss: -314679357.4089 - val_loss: -314848066.1989\n",
      "Epoch 49/100\n",
      "9954/9954 [==============================] - 20s - loss: -314686794.1394 - val_loss: -314852521.7215\n",
      "Epoch 50/100\n",
      "9954/9954 [==============================] - 20s - loss: -314671472.1640 - val_loss: -314852507.4286\n",
      "Epoch 51/100\n",
      "9954/9954 [==============================] - 20s - loss: -314682841.0046 - val_loss: -314852513.0416\n",
      "Epoch 52/100\n",
      "9954/9954 [==============================] - 20s - loss: -314681168.6462 - val_loss: -314852501.6420\n",
      "Epoch 53/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687721.9787 - val_loss: -314852518.8282\n",
      "Epoch 54/100\n",
      "9954/9954 [==============================] - 20s - loss: -314683501.9200 - val_loss: -314852518.8282\n",
      "Epoch 55/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687699.5845 - val_loss: -314852518.8282\n",
      "Epoch 56/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687740.5602 - val_loss: -314852516.1085\n",
      "Epoch 57/100\n",
      "9954/9954 [==============================] - 20s - loss: -314685757.7689 - val_loss: -314852336.7233\n",
      "Epoch 58/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687741.8332 - val_loss: -314852515.9349\n",
      "Epoch 59/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687683.9735 - val_loss: -314852521.7215\n",
      "Epoch 60/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687741.6596 - val_loss: -314852533.4684\n",
      "Epoch 61/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687717.2465 - val_loss: -314852530.4014\n",
      "Epoch 62/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687739.5893 - val_loss: -314852518.8282\n",
      "Epoch 63/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687174.0631 - val_loss: -314852521.7215\n",
      "Epoch 64/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687199.2863 - val_loss: -314852521.7215\n",
      "Epoch 65/100\n",
      "9954/9954 [==============================] - 20s - loss: -314679716.6743 - val_loss: -314852518.8282\n",
      "Epoch 66/100\n",
      "9954/9954 [==============================] - 20s - loss: -314673440.6944 - val_loss: -314852521.7215\n",
      "Epoch 67/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687749.5873 - val_loss: -314852527.5081\n",
      "Epoch 68/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687744.5529 - val_loss: -314852527.5081\n",
      "Epoch 69/100\n",
      "9954/9954 [==============================] - 20s - loss: -314686539.2389 - val_loss: -314852383.1899\n",
      "Epoch 70/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687747.1119 - val_loss: -314852524.6148\n",
      "Epoch 71/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687587.4720 - val_loss: -314852521.8951\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9954/9954 [==============================] - 20s - loss: -314687744.0514 - val_loss: -314852524.9620\n",
      "Epoch 73/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687748.8929 - val_loss: -314852530.5750\n",
      "Epoch 74/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687749.0279 - val_loss: -314852527.5081\n",
      "Epoch 75/100\n",
      "9954/9954 [==============================] - 19s - loss: -314686000.3826 - val_loss: -314852527.8553\n",
      "Epoch 76/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687724.0362 - val_loss: -314752294.5967\n",
      "Epoch 77/100\n",
      "9954/9954 [==============================] - 20s - loss: -314687747.7420 - val_loss: -314852524.9620\n",
      "Epoch 78/100\n",
      "9954/9954 [==============================] - 19s - loss: -314685333.7191 - val_loss: -314852524.6148\n",
      "Epoch 79/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687744.1736 - val_loss: -314852524.6148\n",
      "Epoch 80/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687746.2311 - val_loss: -314852536.1881\n",
      "Epoch 81/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687746.6040 - val_loss: -314852533.2948\n",
      "Epoch 82/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687749.1244 - val_loss: -314852530.4014\n",
      "Epoch 83/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687738.6442 - val_loss: -314852527.6817\n",
      "Epoch 84/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687750.8154 - val_loss: -314852533.2948\n",
      "Epoch 85/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687750.1917 - val_loss: -314852533.4684\n",
      "Epoch 86/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687748.7707 - val_loss: -314852533.4684\n",
      "Epoch 87/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687743.6528 - val_loss: -314852533.4684\n",
      "Epoch 88/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687236.8672 - val_loss: -314852519.1754\n",
      "Epoch 89/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687729.6878 - val_loss: -314852521.7215\n",
      "Epoch 90/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687676.8174 - val_loss: -314852521.7215\n",
      "Epoch 91/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687677.5696 - val_loss: -314852524.6148\n",
      "Epoch 92/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687708.9717 - val_loss: -314852524.7884\n",
      "Epoch 93/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687606.2592 - val_loss: -314852521.8951\n",
      "Epoch 94/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687744.8937 - val_loss: -314852524.6148\n",
      "Epoch 95/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687747.3241 - val_loss: -314852522.0687\n",
      "Epoch 96/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687743.3442 - val_loss: -314852530.5750\n",
      "Epoch 97/100\n",
      "9954/9954 [==============================] - 18s - loss: -314685423.6303 - val_loss: -314852521.8951\n",
      "Epoch 98/100\n",
      "9954/9954 [==============================] - 19s - loss: -314687747.3562 - val_loss: -314852530.5750\n",
      "Epoch 99/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687742.6434 - val_loss: -314852521.8951\n",
      "Epoch 100/100\n",
      "9954/9954 [==============================] - 18s - loss: -314687746.8676 - val_loss: -314852527.5081\n"
     ]
    }
   ],
   "source": [
    "model_a.train_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnaseq_df = pd.concat([rnaseq_train_df, rnaseq_test_df])\n",
    "\n",
    "model_a_compression = model_a.compress(rnaseq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.954180</td>\n",
       "      <td>0.330703</td>\n",
       "      <td>1.377522</td>\n",
       "      <td>0.393589</td>\n",
       "      <td>1.070897</td>\n",
       "      <td>0.293837</td>\n",
       "      <td>0.145054</td>\n",
       "      <td>1.644300</td>\n",
       "      <td>0.469439</td>\n",
       "      <td>0.398456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482913</td>\n",
       "      <td>0.913218</td>\n",
       "      <td>0.315881</td>\n",
       "      <td>0.986681</td>\n",
       "      <td>1.218974</td>\n",
       "      <td>0.488740</td>\n",
       "      <td>1.165367</td>\n",
       "      <td>0.128530</td>\n",
       "      <td>0.520871</td>\n",
       "      <td>0.405544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.950464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.673059</td>\n",
       "      <td>0.613328</td>\n",
       "      <td>1.532724</td>\n",
       "      <td>0.458112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.674594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>2.482697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.401619</td>\n",
       "      <td>0.443003</td>\n",
       "      <td>1.231652</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>0.532845</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.458905</td>\n",
       "      <td>0.068619</td>\n",
       "      <td>2.361578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.672315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038507</td>\n",
       "      <td>1.215361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.707507</td>\n",
       "      <td>0.373794</td>\n",
       "      <td>1.251458</td>\n",
       "      <td>2.299709</td>\n",
       "      <td>0.631258</td>\n",
       "      <td>2.474470</td>\n",
       "      <td>0.111772</td>\n",
       "      <td>0.297639</td>\n",
       "      <td>0.818198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.550024</td>\n",
       "      <td>2.060209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.260229</td>\n",
       "      <td>1.692618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.980341</td>\n",
       "      <td>1.031264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770652</td>\n",
       "      <td>1.646624</td>\n",
       "      <td>2.081531</td>\n",
       "      <td>0.148650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.216234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.494698</td>\n",
       "      <td>2.430187</td>\n",
       "      <td>1.371669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.228052</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>1.043117</td>\n",
       "      <td>0.397331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.051901</td>\n",
       "      <td>1.123032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.582039</td>\n",
       "      <td>0.552501</td>\n",
       "      <td>...</td>\n",
       "      <td>1.242961</td>\n",
       "      <td>0.497624</td>\n",
       "      <td>0.224661</td>\n",
       "      <td>1.292099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.001120</td>\n",
       "      <td>1.308825</td>\n",
       "      <td>0.596963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>0.772927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021058</td>\n",
       "      <td>0.819768</td>\n",
       "      <td>1.373495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500905</td>\n",
       "      <td>0.493829</td>\n",
       "      <td>0.095307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387420</td>\n",
       "      <td>0.134789</td>\n",
       "      <td>0.981281</td>\n",
       "      <td>0.892302</td>\n",
       "      <td>0.915085</td>\n",
       "      <td>1.055817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.166663</td>\n",
       "      <td>1.718126</td>\n",
       "      <td>0.832233</td>\n",
       "      <td>2.179651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.516548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.160103</td>\n",
       "      <td>1.274925</td>\n",
       "      <td>1.503603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>0.314210</td>\n",
       "      <td>1.743268</td>\n",
       "      <td>1.578749</td>\n",
       "      <td>1.629971</td>\n",
       "      <td>0.407299</td>\n",
       "      <td>0.667522</td>\n",
       "      <td>1.372834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.967295</td>\n",
       "      <td>0.510964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599683</td>\n",
       "      <td>1.776488</td>\n",
       "      <td>0.700661</td>\n",
       "      <td>1.280100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.731537</td>\n",
       "      <td>0.611898</td>\n",
       "      <td>0.711631</td>\n",
       "      <td>0.735922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.431769</td>\n",
       "      <td>1.374160</td>\n",
       "      <td>0.491403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498483</td>\n",
       "      <td>1.034472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.515734</td>\n",
       "      <td>0.940312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.245875</td>\n",
       "      <td>1.847153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622123</td>\n",
       "      <td>0.825398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.657990</td>\n",
       "      <td>1.685218</td>\n",
       "      <td>1.738196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>0.669452</td>\n",
       "      <td>0.612372</td>\n",
       "      <td>0.065174</td>\n",
       "      <td>0.842870</td>\n",
       "      <td>1.408338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463884</td>\n",
       "      <td>1.223435</td>\n",
       "      <td>0.369556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.861892</td>\n",
       "      <td>0.894239</td>\n",
       "      <td>0.805977</td>\n",
       "      <td>1.358546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.257264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11060 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3         4         5         6         7    \\\n",
       "0     0.954180  0.330703  1.377522  0.393589  1.070897  0.293837  0.145054   \n",
       "1     0.950464  0.000000  2.673059  0.613328  1.532724  0.458112  0.000000   \n",
       "2     0.458905  0.068619  2.361578  0.000000  0.672315  0.000000  0.038507   \n",
       "3     0.000000  2.550024  2.060209  0.000000  0.000000  1.260229  1.692618   \n",
       "4     0.228052  0.829146  1.043117  0.397331  0.000000  1.051901  1.123032   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1101  0.772927  0.000000  0.021058  0.819768  1.373495  0.000000  0.000000   \n",
       "1102  0.000000  0.000000  0.338145  0.000000  0.000000  0.000000  1.166663   \n",
       "1103  0.314210  1.743268  1.578749  1.629971  0.407299  0.667522  1.372834   \n",
       "1104  0.000000  1.431769  1.374160  0.491403  0.000000  0.498483  1.034472   \n",
       "1105  0.669452  0.612372  0.065174  0.842870  1.408338  0.000000  0.463884   \n",
       "\n",
       "           8         9         10   ...       91        92        93   \\\n",
       "0     1.644300  0.469439  0.398456  ...  0.482913  0.913218  0.315881   \n",
       "1     0.674594  0.000000  0.000000  ...  0.236400  2.482697  0.000000   \n",
       "2     1.215361  0.000000  0.000000  ...  0.000000  1.707507  0.373794   \n",
       "3     0.000000  1.980341  1.031264  ...  0.770652  1.646624  2.081531   \n",
       "4     0.000000  0.582039  0.552501  ...  1.242961  0.497624  0.224661   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1101  1.500905  0.493829  0.095307  ...  0.000000  0.387420  0.134789   \n",
       "1102  1.718126  0.832233  2.179651  ...  0.866011  0.000000  1.516548   \n",
       "1103  0.000000  0.967295  0.510964  ...  0.599683  1.776488  0.700661   \n",
       "1104  0.000000  1.515734  0.940312  ...  0.000000  1.245875  1.847153   \n",
       "1105  1.223435  0.369556  0.000000  ...  0.000000  1.861892  0.894239   \n",
       "\n",
       "           94        95        96        97        98        99        100  \n",
       "0     0.986681  1.218974  0.488740  1.165367  0.128530  0.520871  0.405544  \n",
       "1     0.000000  2.401619  0.443003  1.231652  0.011421  0.532845  0.000000  \n",
       "2     1.251458  2.299709  0.631258  2.474470  0.111772  0.297639  0.818198  \n",
       "3     0.148650  0.000000  1.216234  0.000000  1.494698  2.430187  1.371669  \n",
       "4     1.292099  0.000000  0.000000  0.000000  1.001120  1.308825  0.596963  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1101  0.981281  0.892302  0.915085  1.055817  0.000000  0.000000  0.100387  \n",
       "1102  0.000000  0.507725  0.000000  0.000000  2.160103  1.274925  1.503603  \n",
       "1103  1.280100  0.000000  0.000000  0.731537  0.611898  0.711631  0.735922  \n",
       "1104  0.000000  0.622123  0.825398  0.000000  1.657990  1.685218  1.738196  \n",
       "1105  0.805977  1.358546  0.000000  2.257264  0.000000  0.000000  0.000000  \n",
       "\n",
       "[11060 rows x 100 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a_compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
